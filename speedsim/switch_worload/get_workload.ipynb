{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR 1. Description of HW\n",
    "    - No of GPUS\n",
    "    - topology of GPU/Switches (automation)\n",
    "        - list of destination per fw,inp,wt\n",
    "    - clock per gpu or clock per switch\n",
    "   \n",
    "## AR 2. Workload\n",
    "    - how many instance of workload\n",
    "    - range of gpus to occupy\n",
    "    - for each task, add workload-ins attribute\n",
    "    - write data in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"C:\\\\Users\\\\vr\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Repo\\\\dl-modeling\\\\speedsim\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asap.workload import Workload, Task, TYPES\n",
    "from asap.ips import IP, ExecutingUnit, Driver, Port\n",
    "from asap.hw import Clock\n",
    "from asap.buses import Bus\n",
    "from asap.memories import Memory\n",
    "from asap.system_platform import Platform\n",
    "from asap.schedulers import SystemScheduler\n",
    "from asap.mapping import MappingEntity\n",
    "from speedsim import SpeedSim\n",
    "from post_processing.utils import AnalysisData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_config(fname):\n",
    "    if fname is None:\n",
    "        return None\n",
    "    with open(fname) as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        return {row[0]: row[1] for row in reader if len(row) > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyWords\n",
    "SUCCESSORS = 'successors'\n",
    "PREDECESSORS = 'predecessors'\n",
    "FW = 'FW'\n",
    "INP = 'INP'\n",
    "WT = 'WT'\n",
    "COMM = 'COMM'\n",
    "COMM_FW = COMM + '_FW'\n",
    "COMM_INP = COMM + '_INP'\n",
    "COMM_WT = COMM + '_WT'\n",
    "COMM_SO = COMM + '_SO'\n",
    "COMM_SO_FW = COMM + '_SO_FW'\n",
    "COMM_SO_INP = COMM + '_SO_INP'\n",
    "COMM_SO_WT = COMM + '_SO_WT'\n",
    "GPUs = 1\n",
    "EX_UNITS_PER_GPU = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def _get_source(link, workload_graph):\n",
    "    source = int(link['source'])\n",
    "    return workload_graph['nodes'][source]['label']\n",
    "\n",
    "\n",
    "def _get_target(link, workload_graph):\n",
    "    target = int(link['target'])\n",
    "    return workload_graph['nodes'][target]['label']\n",
    "\n",
    "def speedsim_analysis_train(workload_json, config_dict, scaleout_dict, scaleout=False, include_timeline=False,\n",
    "                            trace_gen=True, network_name=None):\n",
    "\n",
    "    wl_json_fd = open(workload_json, 'r')\n",
    "    wl_json = json.load(wl_json_fd)\n",
    "\n",
    "    workload_cycles = 0\n",
    "    successor_dict = {}\n",
    "    predecessor_dict = {}\n",
    "\n",
    "    last_fwd_source = \"\"\n",
    "\n",
    "    for link in (wl_json[\"links\"]):\n",
    "        source = _get_source(link, wl_json)\n",
    "        dst = _get_target(link, wl_json)\n",
    "        if \"bwd_\" not in source:\n",
    "            last_fwd_source = source\n",
    "        if source not in successor_dict.keys():\n",
    "            successor_dict[source] = []\n",
    "        successor_dict[source].append(dst)\n",
    "        if dst not in predecessor_dict.keys():\n",
    "            predecessor_dict[dst] = []\n",
    "        predecessor_dict[dst].append(source)\n",
    "\n",
    "    # add second iteration of fwd tasks,\n",
    "    # this will act as bwd-fwd tasks\n",
    "    if int(config_dict[\"data_parallel\"]) == 0:\n",
    "        first_fwd_source = list(successor_dict.keys() - predecessor_dict.keys())\n",
    "        temp_keys = list(successor_dict.keys())\n",
    "        for key in temp_keys:\n",
    "            if \"bwd_\" not in key:\n",
    "                if key != last_fwd_source:\n",
    "                    # there mite be layers  which dont have successors\n",
    "                    if key in successor_dict.keys():\n",
    "                        successor_dict[\"2_\" + key] = [\"2_\" + x for x in successor_dict[key]]\n",
    "                    # there mite be layers  which dont have predecessors\n",
    "                    if key in predecessor_dict.keys():\n",
    "                        predecessor_dict[\"2_\" + key] = [\"2_\" + x for x in predecessor_dict[key]]\n",
    "                else:\n",
    "                    if len(successor_dict[key]) == 1:\n",
    "                        successor_dict[\"2_\" + key] = successor_dict[key]\n",
    "                        predecessor_dict[\"2_\" + key] = [\"2_\" + x for x in predecessor_dict[key]]\n",
    "\n",
    "                        predecessor_dict[successor_dict[key][0]] = [\"2_\" + key]\n",
    "                        successor_dict[key] = [\"2_\" + x for x in first_fwd_source]\n",
    "                        for x in first_fwd_source:\n",
    "                            predecessor_dict[\"2_\" + x] = [key]\n",
    "                    else:\n",
    "                        raise Exception(\"unable to construct taskgraph\")\n",
    "\n",
    "    # Going throgh all the layers - create task of layer name\n",
    "    # Through each out/in tensor in each layer, save connections\n",
    "    TASK_NAME_SEPARATOR = '__'\n",
    "    ref_freq = float(config_dict[\"frequency_in_Ghz\"]) * 1000  # 1.5GH = 1500Mhz = 0.000666us\n",
    "    ref_period = 1 / ref_freq  # 1.5GH = 1500Mhz = 0.000666us\n",
    "\n",
    "    msg_size_keys = {'comms_time_fwd_cycles': 'fwd_pass_msg_size',\n",
    "                     'comms_scaleout_time_fwd_cycles': 'fwd_pass_msg_size',\n",
    "                     'comms_time_inp_grad_cycles': 'inp_grad_msg_size',\n",
    "                     'comms_scaleout_time_inp_cycles': 'inp_grad_msg_size',\n",
    "                     'comms_time_wtgrad_cycles': 'wt_grad_msg_size',\n",
    "                     'comms_scaleout_time_wt_cycles': 'wt_grad_msg_size'}\n",
    "\n",
    "    comms_type_keys = {'comms_time_fwd_cycles': 'fwd_collective_comms_type',\n",
    "                       'comms_scaleout_time_fwd_cycles': 'fwd_collective_comms_type',\n",
    "                       'comms_time_inp_grad_cycles': 'inp_collective_comms_type',\n",
    "                       'comms_scaleout_time_inp_cycles': 'inp_collective_comms_type',\n",
    "                       'comms_time_wtgrad_cycles': 'wt_collective_comms_type',\n",
    "                       'comms_scaleout_time_wt_cycles': 'wt_collective_comms_type'}\n",
    "\n",
    "    fwd_layer_sub_tasks = {'fwd_pass_comp_cycles': FW,\n",
    "                           'comms_time_fwd_cycles': COMM_FW,\n",
    "                           'comms_scaleout_time_fwd_cycles': COMM_SO_FW,\n",
    "                           'comms_scaleout_time_wt_cycles': COMM_SO_WT}\n",
    "    bwd_layer_sub_tasks = {'inp_grad_comp_cycles': INP,\n",
    "                           'wt_grad_comp_cycles': WT,\n",
    "                           'comms_time_inp_grad_cycles': COMM_INP,\n",
    "                           'comms_time_wtgrad_cycles': COMM_WT,\n",
    "                           'comms_scaleout_time_inp_cycles': COMM_SO_INP,\n",
    "                           'comms_scaleout_time_wt_cycles': COMM_SO_WT}\n",
    "\n",
    "    # Workload parsing\n",
    "    workload = Workload('Resnet50')\n",
    "    connections = dict()  # {layer -> {predeseccors: [layers], successors: [layers]}\n",
    "    start = Task('Start', TYPES.START)\n",
    "    end = Task('End', TYPES.END)\n",
    "    workload.add_tasks([start, end])\n",
    "\n",
    "    # Parsing layers, creating tasks and internal connections, saving external connections\n",
    "    con = 0\n",
    "    layers_num = len(wl_json['nodes'])\n",
    "    for layer in wl_json['nodes']:\n",
    "        layer_name = layer['data']['Layer']['Layer Name']\n",
    "        layer_pass = layer['data']['Layer']['l_pass']\n",
    "        sub_tasks = dict()\n",
    "        sub_task2 = dict()\n",
    "        if layer_pass == \"bwd\":\n",
    "            for alias, sub_task in bwd_layer_sub_tasks.items():\n",
    "                processing_cycles = float(layer['data']['Layer'][alias])\n",
    "                workload_cycles += processing_cycles\n",
    "                if processing_cycles > 0 or not sub_task.startswith(COMM):\n",
    "                    if sub_task.startswith(COMM):\n",
    "                        proc = Task(layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                    processing_cycles=processing_cycles,\n",
    "                                    msg_size=float(layer['data']['Layer'][msg_size_keys[alias]]),\n",
    "                                    comms_type=layer['data']['Layer'][comms_type_keys[alias]],\n",
    "                                    layer_pass=layer_pass)\n",
    "                    else:\n",
    "                        proc = Task(layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                    processing_cycles=processing_cycles,\n",
    "                                    layer_pass=layer_pass)\n",
    "                    if sub_task.startswith(COMM):\n",
    "                        proc.attach_attribute('TASK_TYPE', COMM)\n",
    "                    sub_tasks[sub_task] = proc\n",
    "\n",
    "                    workload.add_task(proc)\n",
    "\n",
    "            if COMM_INP in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[INP], sub_tasks[COMM_INP])\n",
    "                con += 1\n",
    "                if COMM_SO_INP in sub_tasks.keys():\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_INP], sub_tasks[COMM_SO_INP])\n",
    "                    con += 1\n",
    "            elif COMM_SO_INP in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[INP], sub_tasks[COMM_SO_INP])\n",
    "                con += 1\n",
    "\n",
    "            new_end = Task('End_of_' + layer_name + '_WT', TYPES.END)\n",
    "            workload.add_task(new_end)\n",
    "\n",
    "            if COMM_WT in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[WT], sub_tasks[COMM_WT])\n",
    "                con += 1\n",
    "                if COMM_SO_WT in sub_tasks.keys():\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_WT], sub_tasks[COMM_SO_WT])\n",
    "                    con += 1\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_SO_WT], new_end)\n",
    "                    con += 1\n",
    "                else:\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_WT], new_end)\n",
    "                    con += 1\n",
    "            elif COMM_SO_WT in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[WT], sub_tasks[COMM_SO_WT])\n",
    "                con += 1\n",
    "                workload.connect_tasks(str(con), sub_tasks[COMM_SO_WT], new_end)\n",
    "                con += 1\n",
    "            elif WT in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[WT], new_end)\n",
    "                con += 1\n",
    "            else:\n",
    "                print(\"Could not connect {}\".format(new_end.name))\n",
    "\n",
    "            if WT in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[INP], sub_tasks[WT])\n",
    "                con += 1\n",
    "\n",
    "            # new_end = Task('End_of_' + layer_name + 'COMM_SO', TYPES.END)\n",
    "            # workload.add_task(new_end)\n",
    "            # workload.connect_tasks(str(con), sub_tasks[COMM_SO], new_end)\n",
    "            # con += 1\n",
    "\n",
    "        else:\n",
    "            for alias, sub_task in fwd_layer_sub_tasks.items():\n",
    "                processing_cycles = float(layer['data']['Layer'][alias])\n",
    "                workload_cycles += processing_cycles\n",
    "                # comms which have 0 processing cycles to be ignored,\n",
    "                # INP and WT even though they have 0 processing cycle, create task related to that\n",
    "                if processing_cycles > 0 or not sub_task.startswith(COMM):\n",
    "                    if sub_task.startswith(COMM):\n",
    "                        proc = Task(layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                    processing_cycles=processing_cycles,\n",
    "                                    msg_size=float(layer['data']['Layer'][msg_size_keys[alias]]),\n",
    "                                    comms_type=layer['data']['Layer'][comms_type_keys[alias]],\n",
    "                                    layer_pass=layer_pass)\n",
    "                    else:\n",
    "                        proc = Task(layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                    processing_cycles=processing_cycles,\n",
    "                                    layer_pass=layer_pass)\n",
    "                    if sub_task.startswith(COMM):\n",
    "                        proc.attach_attribute('TASK_TYPE', COMM)\n",
    "                    sub_tasks[sub_task] = proc\n",
    "                    workload.add_task(proc)\n",
    "\n",
    "                    if int(config_dict[\"data_parallel\"]) == 0:\n",
    "                        if sub_task.startswith(COMM):\n",
    "                            proc2 = Task(\"2_\" + layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                         processing_cycles=processing_cycles,\n",
    "                                         msg_size=float(layer['data']['Layer'][msg_size_keys[alias]]),\n",
    "                                         comms_type=layer['data']['Layer'][comms_type_keys[alias]],\n",
    "                                         layer_pass=layer_pass)\n",
    "                        else:\n",
    "                            proc2 = Task(\"2_\" + layer_name + TASK_NAME_SEPARATOR + sub_task, TYPES.PROC,\n",
    "                                         processing_cycles=processing_cycles,\n",
    "                                         layer_pass=layer_pass)\n",
    "                        if sub_task.startswith(COMM):\n",
    "                            proc2.attach_attribute('TASK_TYPE', COMM)\n",
    "                        sub_task2[sub_task] = proc2\n",
    "                        workload.add_task(proc2)\n",
    "\n",
    "            if COMM_FW in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[FW], sub_tasks[COMM_FW])\n",
    "                con += 1\n",
    "                if COMM_SO_FW in sub_tasks.keys():\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_FW], sub_tasks[COMM_SO_FW])\n",
    "                    con += 1\n",
    "                    if COMM_SO_WT in sub_tasks.keys():\n",
    "                        workload.connect_tasks(str(con), sub_tasks[COMM_SO_FW], sub_tasks[COMM_SO_WT])\n",
    "                        con += 1\n",
    "                else:\n",
    "                    if COMM_SO_WT in sub_tasks.keys():\n",
    "                        workload.connect_tasks(str(con), sub_tasks[COMM_FW], sub_tasks[COMM_SO_WT])\n",
    "                        con += 1\n",
    "            elif COMM_SO_FW in sub_tasks.keys():\n",
    "                workload.connect_tasks(str(con), sub_tasks[FW], sub_tasks[COMM_SO_FW])\n",
    "                con += 1\n",
    "                if COMM_SO_WT in sub_tasks.keys():\n",
    "                    workload.connect_tasks(str(con), sub_tasks[COMM_SO_FW], sub_tasks[COMM_SO_WT])\n",
    "                    con += 1\n",
    "            else:\n",
    "                if COMM_SO_WT in sub_tasks.keys():\n",
    "                    workload.connect_tasks(str(con), sub_tasks[FW], sub_tasks[COMM_SO_WT])\n",
    "                    con += 1\n",
    "\n",
    "            if COMM_FW in sub_task2.keys():\n",
    "                workload.connect_tasks(\"2_\" + str(con), sub_task2[FW], sub_task2[COMM_FW])\n",
    "                con += 1\n",
    "                if COMM_SO_FW in sub_task2.keys():\n",
    "                    workload.connect_tasks(\"2_\" + str(con), sub_task2[COMM_FW], sub_task2[COMM_SO_FW])\n",
    "                    con += 1\n",
    "                    if COMM_SO_WT in sub_task2.keys():\n",
    "                        workload.connect_tasks(\"2_\" + str(con), sub_task2[COMM_SO_FW], sub_task2[COMM_SO_WT])\n",
    "                        con += 1\n",
    "                else:\n",
    "                    if COMM_SO_WT in sub_task2.keys():\n",
    "                        workload.connect_tasks(\"2_\" + str(con), sub_task2[COMM_FW], sub_task2[COMM_SO_WT])\n",
    "                        con += 1\n",
    "            elif COMM_SO_FW in sub_task2.keys():\n",
    "                workload.connect_tasks(\"2_\" + str(con), sub_task2[FW], sub_task2[COMM_SO_FW])\n",
    "                con += 1\n",
    "                if COMM_SO_WT in sub_task2.keys():\n",
    "                    workload.connect_tasks(\"2_\" + str(con), sub_task2[COMM_SO_FW], sub_task2[COMM_SO_WT])\n",
    "                    con += 1\n",
    "            else:\n",
    "                if COMM_SO_WT in sub_task2.keys():\n",
    "                    workload.connect_tasks(\"2_\" + str(con), sub_task2[FW], sub_task2[COMM_SO_WT])\n",
    "                    con += 1\n",
    "\n",
    "        # Retreiving successors connections\n",
    "        layer_connections = connections.get(layer_name, dict())\n",
    "        successors = layer_connections.get(SUCCESSORS, [])\n",
    "        if layer_name in successor_dict.keys():\n",
    "            successors.extend(successor_dict[layer_name])\n",
    "        layer_connections[SUCCESSORS] = successors\n",
    "\n",
    "        # Retreiving predecessors connections\n",
    "        predecessors = layer_connections.get(PREDECESSORS, [])\n",
    "        if layer_name in predecessor_dict.keys():\n",
    "            predecessors.extend(predecessor_dict[layer_name])\n",
    "        layer_connections[PREDECESSORS] = predecessors\n",
    "        connections[layer_name] = layer_connections\n",
    "\n",
    "        if layer_pass != \"bwd\" and int(config_dict[\"data_parallel\"]) == 0:\n",
    "            layer_connections2 = connections.get(\"2_\" + layer_name, dict())\n",
    "            successors = layer_connections2.get(SUCCESSORS, [])\n",
    "            if \"2_\" + layer_name in successor_dict.keys():\n",
    "                successors.extend(successor_dict[\"2_\" + layer_name])\n",
    "            layer_connections2[SUCCESSORS] = successors\n",
    "\n",
    "            predecessors = layer_connections2.get(PREDECESSORS, [])\n",
    "            if \"2_\" + layer_name in predecessor_dict.keys():\n",
    "                predecessors.extend(predecessor_dict[\"2_\" + layer_name])\n",
    "            layer_connections2[PREDECESSORS] = predecessors\n",
    "            connections[\"2_\" + layer_name] = layer_connections2\n",
    "\n",
    "    # External connections\n",
    "    for layer, layer_connections in connections.items():\n",
    "        # Successors connections\n",
    "        if \"bwd_\" not in layer:\n",
    "            layer_fw_so_comm_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_SO_FW)\n",
    "            if layer_fw_so_comm_t is None:\n",
    "                layer_fw_so_comm_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_FW)\n",
    "            if layer_fw_so_comm_t is None:\n",
    "                layer_fw_so_comm_t = workload.get_task(layer + TASK_NAME_SEPARATOR + FW)\n",
    "            successors = layer_connections.get(SUCCESSORS, [])\n",
    "            for successor in successors:\n",
    "                if \"bwd_\" not in successor:\n",
    "                    successor_fw_t = workload.get_task(successor + TASK_NAME_SEPARATOR + FW)\n",
    "                    workload.connect_tasks(str(con), layer_fw_so_comm_t, successor_fw_t);\n",
    "                    con += 1\n",
    "                else:\n",
    "                    successor_inp_t = workload.get_task(successor + TASK_NAME_SEPARATOR + INP)\n",
    "                    workload.connect_tasks(str(con), layer_fw_so_comm_t, successor_inp_t)\n",
    "                    con += 1\n",
    "        else:\n",
    "            layer_comm_so_inp_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_SO_INP)\n",
    "            if layer_comm_so_inp_t is None:\n",
    "                layer_comm_so_inp_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_INP)\n",
    "            if layer_comm_so_inp_t is None:\n",
    "                layer_comm_so_inp_t = workload.get_task(layer + TASK_NAME_SEPARATOR + INP)\n",
    "            successors = layer_connections.get(SUCCESSORS, [])\n",
    "            for successor in successors:\n",
    "                if \"bwd_\" in successor:  # redudant check bwd layer successors are always\n",
    "                    successor_inp_t = workload.get_task(successor + TASK_NAME_SEPARATOR + INP)\n",
    "                    workload.connect_tasks(str(con), layer_comm_so_inp_t, successor_inp_t)\n",
    "                    con += 1\n",
    "                else:\n",
    "                    # this is last optimizer layer, which should get triggered after\n",
    "                    # all sub tasks have to be executed before optimizer can kick in\n",
    "                    # as COMM_SO_WT is the last subtask in this TE graph\n",
    "                    layer_comm_so_wt_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_SO_WT)\n",
    "                    if layer_comm_so_wt_t is None:\n",
    "                        layer_comm_so_wt_t = workload.get_task(layer + TASK_NAME_SEPARATOR + COMM_WT)\n",
    "                    if layer_comm_so_wt_t is None:\n",
    "                        layer_comm_so_wt_t = workload.get_task(layer + TASK_NAME_SEPARATOR + WT)\n",
    "                    successor_fw_t = workload.get_task(successor + TASK_NAME_SEPARATOR + FW)\n",
    "                    workload.connect_tasks(str(con), layer_comm_so_wt_t, successor_fw_t)\n",
    "                    con += 1\n",
    "\n",
    "    # layers that dont have predecessors dont have any triggering point\n",
    "    # so they have to be triggered from start\n",
    "    start_layers = list(successor_dict.keys() - predecessor_dict.keys())\n",
    "    for start_layer in start_layers:\n",
    "        start_layer_fw = workload.get_task(start_layer + TASK_NAME_SEPARATOR + FW)\n",
    "        workload.connect_tasks(str(con), start, start_layer_fw)\n",
    "        con += 1\n",
    "\n",
    "    end_layers = list(predecessor_dict.keys() - successor_dict.keys())\n",
    "    for end_layer in end_layers:\n",
    "        end_layer_comm_fw = workload.get_task(end_layer + TASK_NAME_SEPARATOR + COMM_FW)\n",
    "        if end_layer_comm_fw is None:\n",
    "            end_layer_comm_fw = workload.get_task(end_layer + TASK_NAME_SEPARATOR + FW)\n",
    "        workload.connect_tasks(str(con), end_layer_comm_fw, end)\n",
    "        con += 1\n",
    "\n",
    "    workload.draw(os.path.basename(workload_json), view=False, format_='pdf', keep_gv=False)\n",
    "    return workload \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from switch_worload.nw_traffic_gen import nw_traffic_gen\n",
    "config_dict = read_config(\"C:\\\\Users\\\\vr\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Repo\\\\dl-modeling\\\\speedsim\\\\switch_worload\\\\config.csv\")\n",
    "config_scaleout = read_config(\"C:\\\\Users\\\\vr\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Repo\\\\dl-modeling\\\\speedsim\\\\switch_worload\\\\config_scaleout.csv\")\n",
    "workload_json = \"C:\\\\Users\\\\vr\\\\OneDrive - Intel Corporation\\\\Desktop\\\\Repo\\\\dl-modeling\\\\speedsim\\\\switch_worload\\\\ResNet-50.prototxt_graph_out.json\"\n",
    "\n",
    "# number of gpus\n",
    "# ngpu = float(config_scaleout[\"num_pvc\"])\n",
    "ngpu = 32\n",
    "\n",
    "# model parallel\n",
    "# mp = float(config_dict[\"model_split\"])\n",
    "mp = 8\n",
    "\n",
    "#dp = ngpu / mp\n",
    "dp = 8\n",
    "\n",
    "(wt_scaleup, wt_scaleout, fwd_inp_scaleup, fwd_inp_scaleout) = nw_traffic_gen(int(ngpu / dp), int(dp), int(mp))\n",
    "\n",
    "workload = speedsim_analysis_train(workload_json=workload_json,\n",
    "                       config_dict=config_dict,\n",
    "                       scaleout_dict=config_scaleout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend workload to multiple instance\n",
    "\n",
    "tasks = []\n",
    "connections = []\n",
    "\n",
    "workload_instance = int(ngpu / mp)\n",
    "\n",
    "real_workload = Workload(\"Real_Workload\")\n",
    "\n",
    "for task in workload.tasks:\n",
    "    for instance in range(0, workload_instance):\n",
    "        gpu_range = (instance * mp, (instance+1)*mp-1)\n",
    "        attributes = task.attributes\n",
    "        attributes.update({\"workload_instace\": instance,\n",
    "                           \"gpu_range\": gpu_range})\n",
    "        t = Task(str(instance)+\"_\"+task.name,\n",
    "                 type=task.type, read_bytes=task.read_bytes, write_bytes=task.write_bytes,\n",
    "                 processing_cycles=task.processing_cycles, attributes=task.attributes)\n",
    "        real_workload.add_task(t)\n",
    "\n",
    "con = 0\n",
    "for connection in workload.connections:\n",
    "    for instance in range(0, workload_instance):\n",
    "        source = real_workload.get_task(str(instance)+\"_\"+connection.source.name)\n",
    "        target = real_workload.get_task(str(instance)+\"_\"+connection.target.name)\n",
    "        real_workload.connect_tasks(str(con), source, target)   \n",
    "        con+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet-50.prototxt_graph_out.json.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_workload.draw(os.path.basename(\"real_\"+workload_json), view=False, format_='pdf', keep_gv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_workload.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5184"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(real_workload.connections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
